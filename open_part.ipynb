{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFrD0CUr_sh3"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/EXP2.pkl', 'rb') as f: \n",
    "\texp2_data = pickle.load(f)\n",
    "with open('data/EXP3.pkl', 'rb') as f: \n",
    "    exp3_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_neighbors = exp2_data['meta'][0][0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:56:19.741676Z",
     "start_time": "2025-05-09T11:56:06.748038Z"
    },
    "id": "Liosd9lWBdQI"
   },
   "outputs": [],
   "source": [
    "#Let's load the functions from learn_decoder.py\n",
    "from learn_decoder import read_matrix\n",
    "\n",
    "#and the data for experiment 1\n",
    "exp1_fmri = read_matrix(\"data/neuralData_for_EXP1.csv\", sep=\",\", header=True, index_col=True)\n",
    "exp1_vecs = read_matrix(\"data/vectors_180concepts.GV42B300.txt\", sep=\" \")\n",
    "exp1_conc = np.genfromtxt('data/stimuli_180concepts.txt', dtype=np.dtype('U'))  #The names of the 180 concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/lcc/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import sklearn.linear_model\n",
    "from scipy.stats import pearsonr\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function for preparing data for a single voxel ---\n",
    "def get_voxel_specific_data(data, v_idx, voxel_neighbors):\n",
    "    \"\"\"\n",
    "    Extracts data for a central voxel and its 3D neighbors.\n",
    "    \n",
    "    @param data: (n_samples, V_total) array\n",
    "    @param v_idx: index of the central voxel\n",
    "    @param voxel_neighbors: matrix of voxel neighbors, where each row corresponds to a voxel's neighbors.\n",
    "    \n",
    "    Returns: (n_samples, K) array, where K is 1 (center) + number of actual neighbors found.\n",
    "             The central voxel's data is the first column.\n",
    "    \"\"\"\n",
    "    center_voxel_data = data[:, v_idx]\n",
    "    \n",
    "    # Collect neighbors as in the original logic\n",
    "    neighbor_columns = []\n",
    "    neighbors_chosen = set() # Start with the center voxel index to avoid duplicates\n",
    "    neighbors_chosen.add(v_idx) # Ensure the center voxel is included\n",
    "    for n_loop_idx in voxel_neighbors[v_idx]:\n",
    "        if n_loop_idx < 0 or n_loop_idx >= data.shape[1]:\n",
    "            continue\n",
    "        if n_loop_idx not in neighbors_chosen:\n",
    "            neighbors_chosen.add(n_loop_idx)\n",
    "            neighbor_columns.append(data[:, n_loop_idx])\n",
    "    \n",
    "    if not neighbor_columns:\n",
    "        return center_voxel_data[:, np.newaxis] # Return as (n_samples, 1)\n",
    "    else:\n",
    "        return np.column_stack([center_voxel_data] + neighbor_columns)\n",
    "\n",
    "# --- Helper function for processing one voxel within a fold (for parallel execution) ---\n",
    "def process_voxel_for_fold(v_idx, train_data_fold, test_data_fold, \n",
    "                           train_vectors_fold, true_test_vectors_fold,\n",
    "                           voxel_neighbors, ridge_alpha):\n",
    "    \"\"\"\n",
    "    Processes a single voxel: fits ridge, predicts, correlates.\n",
    "    Returns the maximum absolute correlation for this voxel in this fold.\n",
    "    \"\"\"\n",
    "    # 1. Prepare feature matrices for this voxel\n",
    "    # X_train_v will be (n_train_samples, n_features_for_voxel_v)\n",
    "    # n_features_for_voxel_v can vary near edges if window is truncated.\n",
    "    X_train_v = get_voxel_specific_data(train_data_fold, v_idx, voxel_neighbors)\n",
    "    X_test_v = get_voxel_specific_data(test_data_fold, v_idx, voxel_neighbors)\n",
    "\n",
    "    # Basic check, though get_voxel_specific_data should always return at least one column\n",
    "    if X_train_v.shape[1] == 0 or X_test_v.shape[1] == 0 or X_train_v.shape[0] == 0 or X_test_v.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Fit Ridge Regression model\n",
    "    ridge = sklearn.linear_model.Ridge(alpha=ridge_alpha, fit_intercept=True)\n",
    "    try:\n",
    "        ridge.fit(X_train_v, train_vectors_fold)\n",
    "    except ValueError: # Can happen if X_train_v is problematic (e.g. all zeros, too few samples/features)\n",
    "        return 0.0\n",
    "\n",
    "    # 3. Predict semantic vectors for the test data\n",
    "    predicted_vectors = ridge.predict(X_test_v) # Shape: (n_test_samples, n_semantic_dimensions)\n",
    "\n",
    "    # 4. Vectorized Pearson Correlation\n",
    "    # P = predicted_vectors, T = true_test_vectors_fold\n",
    "    # Both are (n_samples_test, n_semantic_dims)\n",
    "    \n",
    "    if predicted_vectors.shape[0] == 0: # No test samples\n",
    "        return 0.0\n",
    "\n",
    "    P_demeaned = predicted_vectors - np.mean(predicted_vectors, axis=0, keepdims=True)\n",
    "    T_demeaned = true_test_vectors_fold - np.mean(true_test_vectors_fold, axis=0, keepdims=True)\n",
    "\n",
    "    numerator = np.sum(P_demeaned * T_demeaned, axis=0)\n",
    "    \n",
    "    ss_P = np.sum(P_demeaned**2, axis=0)\n",
    "    ss_T = np.sum(T_demeaned**2, axis=0)\n",
    "    \n",
    "    denominator = np.sqrt(ss_P * ss_T)\n",
    "    \n",
    "    correlations_all_dims = np.zeros_like(denominator) # Initialize with zeros\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    valid_mask = denominator > 1e-12 # Check if denominator is meaningfully non-zero\n",
    "    \n",
    "    correlations_all_dims[valid_mask] = numerator[valid_mask] / denominator[valid_mask]\n",
    "    \n",
    "    # Handle any potential NaNs that might arise from input NaNs or perfect zero variance in original data\n",
    "    correlations_all_dims = np.nan_to_num(correlations_all_dims, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    if correlations_all_dims.size == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return np.max(np.abs(correlations_all_dims))\n",
    "\n",
    "\n",
    "# --- Main Function ---\n",
    "def voxel_representativeness_optimized(data, vectors, voxel_neighbors, n_jobs: int = -1) -> np.ndarray:\n",
    "    \"\"\" Given a CxV matrix of C concepts and V voxel activations,\n",
    "    return a V-dimensional vector of voxel representativeness.\n",
    "    Optimized for speed using joblib parallelism and vectorized correlation.\n",
    "    \n",
    "    @param data: (S, V) numpy array of S samples (e.g., trials/timepoints) and V voxel activations.\n",
    "    @param vectors: (S, C) numpy array of S samples and C semantic features/dimensions.\n",
    "    @param voxel_neighbors: A matrix where each row corresponds to a voxel and contains indices of its neighbors.\n",
    "    @param n_jobs: Number of parallel jobs for joblib. -1 means use all available cores.\n",
    "    @return: A V-dimensional vector of voxel representativeness scores.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3) # For reproducible fold splitting\n",
    "\n",
    "    n_samples = data.shape[0]\n",
    "    total_n_voxels = data.shape[1] # V\n",
    "    \n",
    "    ridge_alpha = 1.0 # As specified: regularization parameter set to 1\n",
    "\n",
    "    # Splitting the data into 18 folds\n",
    "    n_folds = 18\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    folds = np.array_split(indices, n_folds)\n",
    "\n",
    "    # Initialize a vector to hold the maximum correlation for each voxel across all folds\n",
    "    max_correlation_across_folds = np.zeros(total_n_voxels)\n",
    "\n",
    "    for fold_idx, test_fold_indices in enumerate(tqdm(folds, desc=\"Processing Folds\", unit=\"fold\")):\n",
    "        train_indices = np.setdiff1d(indices, test_fold_indices)\n",
    "\n",
    "        # Create training and test sets for this fold\n",
    "        # Data: (samples, voxels)\n",
    "        # Vectors: (samples, semantic_dims)\n",
    "        current_train_data = data[train_indices, :]\n",
    "        current_test_data = data[test_fold_indices, :]\n",
    "        current_train_vectors = vectors[train_indices, :]\n",
    "        current_true_test_vectors = vectors[test_fold_indices, :]\n",
    "\n",
    "        if current_train_data.shape[0] == 0 or current_test_data.shape[0] == 0:\n",
    "            print(f\"Warning: Fold {fold_idx+1} has no train or test samples, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Parallel processing of voxels for the current fold\n",
    "        # tqdm can be used with joblib, but requires a bit more setup for nested progress bars.\n",
    "        # For simplicity here, tqdm is on the outer (fold) loop.\n",
    "        # The `desc` in the inner loop of the original code will be lost, \n",
    "        # but joblib will utilize cores effectively.\n",
    "        \n",
    "        # print(f\"Fold {fold_idx+1}/{n_folds}: Processing {total_n_voxels} voxels with {n_jobs} workers...\")\n",
    "        \n",
    "        voxel_correlations_for_this_fold = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_voxel_for_fold)(\n",
    "                v_idx, # Index of the voxel to process\n",
    "                current_train_data,\n",
    "                current_test_data,\n",
    "                current_train_vectors,\n",
    "                current_true_test_vectors,\n",
    "                voxel_neighbors,\n",
    "                ridge_alpha\n",
    "            ) for v_idx in range(total_n_voxels) # Iterate over all voxel indices\n",
    "        )\n",
    "        \n",
    "        # Update the max_correlation_across_folds\n",
    "        # voxel_correlations_for_this_fold is a list of max correlations, one for each voxel in this fold\n",
    "        for v_idx, fold_corr in enumerate(voxel_correlations_for_this_fold):\n",
    "            max_correlation_across_folds[v_idx] = max(max_correlation_across_folds[v_idx], fold_corr)\n",
    "\n",
    "    return max_correlation_across_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Folds: 100%|██████████| 18/18 [37:29<00:00, 124.97s/fold]\n"
     ]
    }
   ],
   "source": [
    "# We want to lower the dimensionality of the data to 15000 most informative features\n",
    "# Because it is unclear which features are most informative, we will use the 15000 features with the highest\n",
    "# overall average values in absolute value.\n",
    "def get_top_features(data, n_features=None, metric='mean', **kwargs):\n",
    "    \"\"\"\n",
    "    Select the top n_features based on the mean absolute value of each feature across all samples.\n",
    "    :param data: The input data matrix (samples x features).\n",
    "    :param n_features: The number of top features to select.\n",
    "    :param metric: The metric to use for feature selection \n",
    "    ('mean' or 'var' for variance or \n",
    "    'corr' for max correlation, \n",
    "    's_corr' for simple version).\n",
    "    :return: The reduced data matrix and the indices of the selected features.\n",
    "    \"\"\"\n",
    "    if n_features is None:\n",
    "        return data, np.arange(data.shape[1])\n",
    "\n",
    "    if metric not in ['mean', 'var', 'corr']:\n",
    "        raise ValueError(\"Metric must be one of the allowed ones.\")\n",
    "    elif metric == 'corr':\n",
    "        representativeness = voxel_representativeness_optimized(data, glove_vectors, voxel_neighbors, n_jobs=6)\n",
    "    elif metric == 'var':\n",
    "        representativeness = np.var(data, axis=0)\n",
    "    else:\n",
    "        representativeness = np.mean(np.abs(data), axis=0)\n",
    "    top_indices = np.argsort(representativeness)[-n_features:]\n",
    "    # save the representativeness variable to avoid recomputing it\n",
    "    np.save('data/representativeness.npy', representativeness)\n",
    "\n",
    "    return data[:, top_indices], top_indices\n",
    "\n",
    "exp1_fmri_reduced, top_indices = get_top_features(exp1_fmri, n_features=5000, metric='corr')\n",
    "# Save the reduced data and the top indices\n",
    "np.save('data/data_reduced.npy', exp1_fmri_reduced)\n",
    "np.save('data/top_indices.npy', top_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we need to load the data later\n",
    "# data_reduced = np.load('data/data_reduced.npy')\n",
    "# top_indices = np.load('data/top_indices.npy')\n",
    "# representativeness = np.load('data/representativeness.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we need to change the number of features later\n",
    "representativeness = np.load('data/representativeness.npy')\n",
    "top_indices = np.argsort(representativeness)[-5000:]\n",
    "# to take all\n",
    "# top_indices = np.argsort(representativeness)\n",
    "exp1_fmri_reduced = exp1_fmri[:, top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:56:22.736113Z",
     "start_time": "2025-05-09T11:56:22.733930Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRI_G2E3jWIb",
    "outputId": "43547b69-eb1a-4f97-977d-a639eb22aa08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 185866)\n",
      "(180, 5000)\n",
      "(180, 300)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "print(exp1_fmri.shape)\n",
    "print(exp1_fmri_reduced.shape)\n",
    "print(exp1_vecs.shape)\n",
    "print(exp1_conc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_fmri = exp2_data['Fmridata']\n",
    "exp2_fmri_reduced = exp2_fmri[:, top_indices]\n",
    "\n",
    "exp3_fmri = exp3_data['Fmridata']\n",
    "exp3_fmri_reduced = exp3_fmri[:, top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 185866)\n",
      "(384, 5000)\n",
      "(243, 185866)\n",
      "(243, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(exp2_fmri.shape)\n",
    "print(exp2_fmri_reduced.shape)\n",
    "print(exp3_fmri.shape)\n",
    "print(exp3_fmri_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the vectors for the seconds and third experiments\n",
    "exp2_vecs = read_matrix(\"data/vectors_384sentences.GV42B300.average.txt\", sep=\" \")\n",
    "exp3_vecs = read_matrix(\"data/vectors_243sentences.GV42B300.average.txt\", sep=\" \")\n",
    "# importing the corresponding sentences\n",
    "exp2_sent = np.genfromtxt('data/stimuli_384sentences.txt', delimiter='\\t', dtype=np.dtype('U'))\n",
    "exp3_sent = np.genfromtxt('data/stimuli_243sentences.txt', delimiter='\\t', dtype=np.dtype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 300)\n",
      "(243, 300)\n",
      "(384,)\n",
      "(243,)\n"
     ]
    }
   ],
   "source": [
    "print(exp2_vecs.shape)\n",
    "print(exp3_vecs.shape)\n",
    "print(exp2_sent.shape)\n",
    "print(exp3_sent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/lcc/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the glove vectors from the data/glove.42B.300d.txt\n",
    "glove_embs = {}\n",
    "# if the embedding are already saved, we can load them\n",
    "if os.path.exists('data/glove_embs.pkl'):\n",
    "    with open('data/glove_embs.pkl', 'rb') as f:\n",
    "        glove_embs = pickle.load(f)\n",
    "else:\n",
    "    # otherwise, we load them from the file\n",
    "    with open('data/glove.42B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            vector = np.array([float(x) for x in split_line[1:]])\n",
    "            glove_embs[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the glove embeddings\n",
    "with open('data/glove_embs.pkl', 'wb') as f:\n",
    "    pickle.dump(glove_embs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_sequence_embedding(text_sequence, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Calculates the semantic embedding for a sequence of text by averaging the GloVe word vectors.\n",
    "    \"\"\"\n",
    "    words = text_sequence.split()\n",
    "    word_vectors = [glove_embs[word] for word in words if word in glove_embs]\n",
    "    if not word_vectors:\n",
    "        # If no valid word vectors are found, return a zero vector.\n",
    "        return np.zeros(300)\n",
    "    # Average the word vectors to get the sequence embedding.\n",
    "    sequence_embedding = np.mean(word_vectors, axis=0)\n",
    "    return sequence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_sequence_embedding(text_sequence, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Calculates the semantic embedding for a sequence of text averaging the hidden states of a language model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text_sequence, return_tensors='pt', padding=True)\n",
    "    with torch.no_grad():\n",
    "        # Get the hidden states from the model output.\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # We take the hidden states from the last layer.\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        # Average the token embeddings to get a single vector for the sequence.\n",
    "        sequence_embedding = torch.mean(hidden_states, dim=1).squeeze().numpy()\n",
    "    return sequence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(model_name='mistralai/Mistral-7B-v0.3'):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained language model and tokenizer using AutoClass.\n",
    "\n",
    "    AutoTokenizer and AutoModelForCausalLM are \"smart\" loaders that automatically\n",
    "    infer the correct architecture from the model name and download the\n",
    "    appropriate classes, weights, and configuration. This makes the code\n",
    "    flexible enough to work with Mistral, Llama, GPT-2, Pythia, etc.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_name} model and tokenizer...\")\n",
    "    # You can now use any compatible model from the Hugging Face Hub.\n",
    "    \n",
    "    # The AutoTokenizer will load the correct tokenizer for the Mistral model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # It's good practice to set a padding token if the model doesn't have one.\n",
    "    # The end-of-sentence token is often used for this purpose.\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    # The AutoModelForCausalLM will load the correct model architecture.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        # Using torch_dtype=torch.float16 can help save memory on GPUs.\n",
    "        # Remove this line if you are running on a CPU or encounter issues.\n",
    "        # torch_dtype=torch.float16,\n",
    "        # This will try to spread the model across available GPUs/CPU memory.\n",
    "        # device_map='auto'\n",
    "    )\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_sentence(encoder, target_voxel_activation, model, tokenizer, get_sequence_embedding, max_length=20, k=10):\n",
    "    \"\"\"\n",
    "    Reconstructs a sentence using the Brain-Guided Reconstruction method.\n",
    "\n",
    "    Args:\n",
    "        encoder (np.array): The brain encoder matrix (num_voxels, embedding_dim).\n",
    "        target_voxel_activation (np.array): The actual fMRI data for the sentence.\n",
    "        model: The pre-trained language model.\n",
    "        tokenizer: The tokenizer for the language model.\n",
    "        get_sequence_embedding (function): Function to get the semantic embedding of a text sequence.\n",
    "        max_length (int): The maximum number of words to generate.\n",
    "        k (int): The number of candidate words to evaluate at each step.\n",
    "\n",
    "    Returns:\n",
    "        str: The reconstructed sentence.\n",
    "    \"\"\"\n",
    "    # Start with the model's beginning-of-sequence token.\n",
    "    reconstructed_indices = [tokenizer.bos_token_id]\n",
    "    \n",
    "    print(\"\\n--- Starting Sentence Reconstruction ---\")\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        # Convert the current list of token indices to a tensor for the model.\n",
    "        current_sequence_tensor = torch.tensor([reconstructed_indices])\n",
    "        \n",
    "        # Get the model's predictions (logits) for the next word.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(current_sequence_tensor)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Get the top 'k' most likely next tokens from the language model such that\n",
    "        # they are not special symbols or punctuation.\n",
    "        token_sorted_indices = torch.argsort(next_token_logits, descending=True)\n",
    "        token_sorted_indices = token_sorted_indices[0, :].tolist()\n",
    "        # Filter out tokens that are not valid words (e.g., special symbols, punctuation).\n",
    "        top_k_tokens = []\n",
    "        for token in token_sorted_indices:\n",
    "            if len(top_k_tokens) >= k:\n",
    "                break\n",
    "            # If the token is an eos token, we want to add it too\n",
    "            if token == tokenizer.eos_token_id:\n",
    "                top_k_tokens.append(token)\n",
    "                continue\n",
    "            # Decode the token to check if it's a word.\n",
    "            decoded_token = tokenizer.decode(token).strip()\n",
    "            # We strip whitespace and check if the result is purely alphabetic.\n",
    "            if decoded_token.isalpha():\n",
    "                top_k_tokens.append(token)\n",
    "\n",
    "        best_token = -1\n",
    "        highest_score = -np.inf  # Start with negative infinity.\n",
    "        \n",
    "        # This is the core loop where the brain data guides the generation.\n",
    "        for token in top_k_tokens:\n",
    "            # Create the candidate sequence of token indices.\n",
    "            candidate_indices = reconstructed_indices + [token]\n",
    "            \n",
    "            # Decode the indices to a text string.\n",
    "            candidate_text = tokenizer.decode(candidate_indices)\n",
    "            \n",
    "            # 1. Get the semantic vector for the candidate sequence.\n",
    "            v_candidate = get_sequence_embedding(candidate_text, tokenizer, model)\n",
    "            \n",
    "            # 2. Use the encoder to predict the fMRI pattern for this candidate.\n",
    "            predicted_voxel_activation = v_candidate @ encoder\n",
    "            \n",
    "            # 3. Score the prediction against the actual fMRI data.\n",
    "            # Pearson correlation is a good choice for comparing voxel patterns.\n",
    "            # We add a small epsilon to avoid division by zero if variance is zero.\n",
    "            epsilon = 1e-8\n",
    "            score = np.corrcoef(predicted_voxel_activation + epsilon, target_voxel_activation + epsilon)[0, 1]\n",
    "            \n",
    "            # Keep track of the token that produces the best score.\n",
    "            if score > highest_score:\n",
    "                highest_score = score\n",
    "                best_token = token\n",
    "        \n",
    "        # If no valid token was found, we can stop.\n",
    "        if best_token == -1:\n",
    "            print(\"No valid token found, stopping reconstruction.\")\n",
    "            break\n",
    "\n",
    "        # Append the best token to our sequence.\n",
    "        reconstructed_indices.append(best_token)\n",
    "        \n",
    "        # If the model generates an end-of-sentence token, we can stop.\n",
    "        if best_token == tokenizer.eos_token_id:\n",
    "            print(\"End-of-sentence token generated.\")\n",
    "            break\n",
    "            \n",
    "        # Print the progress for this step.\n",
    "        current_text = tokenizer.decode(reconstructed_indices)\n",
    "        print(f\"Step {i+1}/{max_length}: Best Score={highest_score:.4f}, Current Text: '{current_text}'\")\n",
    "\n",
    "    # Decode the final list of indices into a human-readable sentence.\n",
    "    final_sentence = tokenizer.decode(reconstructed_indices, skip_special_tokens=True)\n",
    "    print(\"--- Reconstruction Complete ---\")\n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EleutherAI/pythia-1b-deduped model and tokenizer...\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the model and tokenizer\n",
    "lm_model, lm_tokenizer = setup_model_and_tokenizer('EleutherAI/pythia-1b-deduped')\n",
    "# The embedding dimension for Pythia-70M is 768.\n",
    "embedding_dim = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings to Sentence Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn_decoder import learn_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the encoder from the data used in the second experiment\n",
    "encoder = learn_encoder(voxels=exp2_fmri_reduced, vectors=exp2_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Sentence: An accordion is a portable musical instrument with two keyboards.\n",
      "\n",
      "--- Starting Sentence Reconstruction ---\n",
      "Step 1/20: Best Score=0.0000, Current Text: '<|endoftext|>s'\n",
      "Step 2/20: Best Score=-0.2235, Current Text: '<|endoftext|>s on'\n",
      "Step 3/20: Best Score=-0.2160, Current Text: '<|endoftext|>s on his'\n",
      "Step 4/20: Best Score=-0.2029, Current Text: '<|endoftext|>s on his personal'\n",
      "Step 5/20: Best Score=-0.2029, Current Text: '<|endoftext|>s on his personal Facebook'\n",
      "Step 6/20: Best Score=-0.1963, Current Text: '<|endoftext|>s on his personal Facebook feed'\n",
      "Step 7/20: Best Score=-0.2044, Current Text: '<|endoftext|>s on his personal Facebook feed on'\n",
      "Step 8/20: Best Score=-0.2044, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday'\n",
      "Step 9/20: Best Score=-0.2110, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night'\n",
      "Step 10/20: Best Score=-0.2154, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at'\n",
      "Step 11/20: Best Score=-0.2145, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his'\n",
      "Step 12/20: Best Score=-0.2150, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his apartment'\n",
      "Step 13/20: Best Score=-0.2156, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his apartment building'\n",
      "Step 14/20: Best Score=-0.2167, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his apartment building complex'\n",
      "Step 15/20: Best Score=-0.2182, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his apartment building complex on'\n",
      "Step 16/20: Best Score=-0.2182, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his apartment building complex on West'\n",
      "Step 17/20: Best Score=-0.2182, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his apartment building complex on West Main'\n",
      "Step 18/20: Best Score=-0.2157, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his apartment building complex on West Main street'\n",
      "Step 19/20: Best Score=-0.2179, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his apartment building complex on West Main street at'\n",
      "Step 20/20: Best Score=-0.2176, Current Text: '<|endoftext|>s on his personal Facebook feed on Tuesday night at his apartment building complex on West Main street at his'\n",
      "--- Reconstruction Complete ---\n",
      "Reconstructed Sentence: s on his personal Facebook feed on Tuesday night at his apartment building complex on West Main street at his\n"
     ]
    }
   ],
   "source": [
    "# for the sake of testing, let's reconstruct the first sentence from the second experiment\n",
    "target_sentence = exp2_sent[0]\n",
    "target_voxel_activation = exp2_fmri_reduced[0, :]\n",
    "print(f\"Target Sentence: {target_sentence}\")\n",
    "\n",
    "reconstructed_sentence = reconstruct_sentence(\n",
    "    encoder=encoder,\n",
    "    target_voxel_activation=target_voxel_activation,\n",
    "    model=lm_model,\n",
    "    tokenizer=lm_tokenizer,\n",
    "    get_sequence_embedding=get_word_sequence_embedding,\n",
    "    max_length=20,\n",
    "    k=10\n",
    ")\n",
    "print(f\"Reconstructed Sentence: {reconstructed_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence embeddings to Sentence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
